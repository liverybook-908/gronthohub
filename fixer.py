"""
Advanced Fixer Bot - Database Cleanup & Integrity Checker
==========================================================
‡¶è‡¶á ‡¶¨‡¶ü Firestore ‡¶°‡¶æ‡¶ü‡¶æ‡¶¨‡ßá‡¶ú‡ßá‡¶∞ ‡¶¨‡¶á‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶°‡¶æ‡¶ü‡¶æ ‡¶á‡¶®‡¶ü‡ßá‡¶ó‡ßç‡¶∞‡¶ø‡¶ü‡¶ø ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶ï‡¶∞‡ßá‡•§

Features:
- Parallel processing with threading
- Smart filtering (only checks old records)
- Retry logic for failed requests
- Comprehensive error handling
- Detailed statistics and reporting
"""

import firebase_admin
from firebase_admin import credentials, firestore
import requests
import os
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass, field
from enum import Enum
import concurrent.futures
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ================================
# ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏
# ================================

class BookStatus(Enum):
    """‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏"""
    PUBLISHED = "published"
    SCHEDULED = "scheduled"
    BROKEN = "broken"
    PENDING = "pending"
    ARCHIVED = "archived"
    DRAFT = "draft"

@dataclass
class FixerConfig:
    """Fixer Bot ‡¶è‡¶∞ ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶®"""
    # Network settings
    request_timeout: int = 15
    retry_attempts: int = 3
    
    # Rate limiting
    rate_limit_delay: float = 0.5  # Archive.org ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø delay
    
    # Parallel processing
    max_workers: int = 4  # ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã thread ‡¶ö‡¶≤‡¶¨‡ßá
    enable_parallel: bool = True  # Parallel processing ‡¶ö‡¶æ‡¶≤‡ßÅ/‡¶¨‡¶®‡ßç‡¶ß
    
    # Smart filtering
    check_interval_days: int = 7  # ‡¶ï‡¶§ ‡¶¶‡¶ø‡¶® ‡¶™‡¶∞ ‡¶™‡¶∞ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶¨‡ßá
    enable_smart_filter: bool = True  # ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶™‡ßÅ‡¶∞‡¶®‡ßã ‡¶∞‡ßá‡¶ï‡¶∞‡ßç‡¶° ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶¨‡ßá
    
    # Batch processing
    batch_size: Optional[int] = None  # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡¶á ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶¨‡ßá (None = ‡¶∏‡¶¨)
    max_execution_time: int = 300  # Maximum execution time (5 minutes)
    
    # Data validation
    default_category: str = "‡¶â‡¶™‡¶®‡ßç‡¶Ø‡¶æ‡¶∏"
    default_status: str = BookStatus.PUBLISHED.value  # PENDING ‡¶è‡¶∞ ‡¶¨‡¶¶‡¶≤‡ßá PUBLISHED
    required_fields: List[str] = field(default_factory=lambda: [
        'bangla_title',
        'category',
        'download_url',
        'status'
    ])
    
    # Logging
    verbose: bool = True  # Detailed logs ‡¶ö‡¶æ‡¶á‡¶≤‡ßá True

@dataclass
class FixerStats:
    """Fixer Bot ‡¶è‡¶∞ ‡¶™‡¶∞‡¶ø‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶®"""
    total_scanned: int = 0
    working_links: int = 0
    broken_links: int = 0
    fixed_records: int = 0
    skipped_records: int = 0
    errors: int = 0
    timeout_stop: bool = False
    execution_time: float = 0.0
    
    def print_report(self):
        """‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ"""
        print("\n" + "="*60)
        print("üìä FIXER BOT - EXECUTION REPORT")
        print("="*60)
        print(f"üîç Total Books Scanned    : {self.total_scanned}")
        print(f"‚úÖ Working Links          : {self.working_links}")
        print(f"‚ùå Broken Links Found     : {self.broken_links}")
        print(f"üîß Records Fixed          : {self.fixed_records}")
        print(f"‚è≠Ô∏è  Records Skipped        : {self.skipped_records}")
        print(f"‚ö†Ô∏è  Errors Encountered    : {self.errors}")
        print(f"‚è±Ô∏è  Execution Time        : {self.execution_time:.2f}s")
        
        if self.timeout_stop:
            print(f"‚è∞ Stopped due to timeout limit")
        
        if self.total_scanned > 0:
            success_rate = (self.working_links / self.total_scanned) * 100
            print(f"\nüìà Success Rate           : {success_rate:.2f}%")
        
        print("="*60 + "\n")

# ================================
# ‡¶≤‡¶ó‡¶ø‡¶Ç ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™
# ================================

def setup_logging(verbose: bool = True):
    """Professional logging ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™"""
    level = logging.INFO if verbose else logging.WARNING
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    return logging.getLogger(__name__)

logger = None  # Will be initialized in main

# ================================
# ‡¶´‡¶æ‡¶Ø‡¶º‡¶æ‡¶∞‡¶¨‡ßá‡¶∏ ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™
# ================================

def initialize_firebase() -> firestore.Client:
    """‡¶´‡¶æ‡¶Ø‡¶º‡¶æ‡¶∞‡¶¨‡ßá‡¶∏ ‡¶á‡¶®‡¶ø‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡¶æ"""
    try:
        firebase_keys_json = os.getenv("FIREBASE_KEYS")
        
        if not firebase_keys_json:
            raise ValueError("FIREBASE_KEYS environment variable not found")
        
        # Temporary file ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
        with open("firebase-key.json", "w") as f:
            f.write(firebase_keys_json)
        
        if not firebase_admin._apps:
            cred = credentials.Certificate("firebase-key.json")
            firebase_admin.initialize_app(cred)
            logger.info("‚úÖ Firebase initialized successfully")
        
        return firestore.client()
    
    except Exception as e:
        logger.error(f"‚ùå Firebase initialization failed: {e}")
        raise

# ================================
# HTTP ‡¶∏‡ßá‡¶∂‡¶® ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ (Retry Logic ‡¶∏‡¶π)
# ================================

def create_session(config: FixerConfig) -> requests.Session:
    """Retry logic ‡¶∏‡¶π HTTP session ‡¶§‡ßà‡¶∞‡¶ø"""
    session = requests.Session()
    
    retry_strategy = Retry(
        total=config.retry_attempts,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET"]
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

# ================================
# ‡¶≤‡¶ø‡¶Ç ‡¶ö‡ßá‡¶ï‡¶æ‡¶∞ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏
# ================================

class LinkChecker:
    """Advanced link checking functionality"""
    
    def __init__(self, config: FixerConfig):
        self.config = config
        self.session = create_session(config)
    
    def check_link(self, url: str) -> Tuple[bool, Optional[str]]:
        """
        ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏ ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡¶æ
        Returns: (is_working, error_message)
        """
        if not url:
            return False, "URL missing"
        
        try:
            response = self.session.head(
                url,
                allow_redirects=True,
                timeout=self.config.request_timeout
            )
            
            if response.status_code == 200:
                return True, None
            else:
                return False, f"Status code: {response.status_code}"
        
        except requests.Timeout:
            return False, "Request timeout"
        except requests.ConnectionError:
            return False, "Connection error"
        except Exception as e:
            return False, f"Unknown error: {str(e)[:50]}"
    
    def __del__(self):
        """Session close ‡¶ï‡¶∞‡¶æ"""
        if hasattr(self, 'session'):
            self.session.close()

# ================================
# ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶ü‡¶∞ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏
# ================================

class DataValidator:
    """‡¶°‡¶æ‡¶ü‡¶æ ‡¶á‡¶®‡¶ü‡ßá‡¶ó‡ßç‡¶∞‡¶ø‡¶ü‡¶ø ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ"""
    
    def __init__(self, config: FixerConfig):
        self.config = config
    
    def validate_book(self, book_data: Dict) -> Dict[str, any]:
        """
        ‡¶¨‡¶á‡¶Ø‡¶º‡ßá‡¶∞ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶´‡¶ø‡¶ï‡ßç‡¶∏‡¶° ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡¶æ
        Returns: Dictionary of fields to update
        """
        updates = {}
        
        # Required fields ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
        for field in self.config.required_fields:
            if field not in book_data or not book_data[field]:
                if field == 'category':
                    updates['category'] = self.config.default_category
                elif field == 'status':
                    # PENDING ‡¶è‡¶∞ ‡¶¨‡¶¶‡¶≤‡ßá PUBLISHED ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø (‡¶´‡ßç‡¶∞‡¶®‡ßç‡¶ü‡¶è‡¶®‡ßç‡¶°‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)
                    updates['status'] = self.config.default_status
        
        # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ ‡¶∏‡ßç‡¶™‡ßá‡¶∏ ‡¶ü‡ßç‡¶∞‡¶ø‡¶Æ ‡¶ï‡¶∞‡¶æ
        if 'bangla_title' in book_data and book_data['bangla_title']:
            trimmed_title = book_data['bangla_title'].strip()
            if trimmed_title != book_data['bangla_title']:
                updates['bangla_title'] = trimmed_title
        
        # ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤ ‡¶ö‡ßá‡¶ï
        if 'english_title' in book_data and book_data['english_title']:
            trimmed_eng = book_data['english_title'].strip()
            if trimmed_eng != book_data['english_title']:
                updates['english_title'] = trimmed_eng
        
        # ‡¶≤‡ßá‡¶ñ‡¶ï‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶ü‡ßç‡¶∞‡¶ø‡¶Æ
        if 'author' in book_data and book_data['author']:
            trimmed_author = book_data['author'].strip()
            if trimmed_author != book_data['author']:
                updates['author'] = trimmed_author
        
        return updates

# ================================
# ‡¶Æ‡ßá‡¶á‡¶® ‡¶´‡¶ø‡¶ï‡ßç‡¶∏‡¶æ‡¶∞ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏
# ================================

class FixerBot:
    """Main Fixer Bot class"""
    
    def __init__(self, config: FixerConfig = None):
        self.config = config or FixerConfig()
        self.db = initialize_firebase()
        self.link_checker = LinkChecker(self.config)
        self.data_validator = DataValidator(self.config)
        self.stats = FixerStats()
        self.start_time = None
    
    def _should_stop_execution(self) -> bool:
        """Timeout check ‡¶ï‡¶∞‡¶æ"""
        if self.start_time is None:
            return False
        
        elapsed = time.time() - self.start_time
        if elapsed >= self.config.max_execution_time:
            logger.warning(f"‚è∞ Timeout reached ({elapsed:.0f}s). Stopping execution...")
            self.stats.timeout_stop = True
            return True
        return False
    
    def process_book(self, doc) -> None:
        """‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶á ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ"""
        try:
            book_data = doc.to_dict()
            doc_id = doc.id
            self.stats.total_scanned += 1
            
            book_title = book_data.get('bangla_title', 'Unknown')
            
            if self.config.verbose:
                logger.info(f"üîç [{self.stats.total_scanned}] Checking: {book_title}")
            
            updates = {}
            
            # ‡ßß. ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
            pdf_url = book_data.get('download_url')
            if pdf_url:
                is_working, error_msg = self.link_checker.check_link(pdf_url)
                
                if not is_working:
                    logger.warning(f"‚ùå Broken link for '{book_title}': {error_msg}")
                    updates['status'] = BookStatus.BROKEN.value
                    updates['error_message'] = error_msg
                    self.stats.broken_links += 1
                else:
                    if self.config.verbose:
                        logger.info(f"‚úÖ Link working for '{book_title}'")
                    self.stats.working_links += 1
                    
                    # ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶ó‡ßá broken ‡¶õ‡¶ø‡¶≤, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ
                    if book_data.get('status') == BookStatus.BROKEN.value:
                        updates['status'] = BookStatus.PUBLISHED.value
                        if 'error_message' in book_data:
                            updates['error_message'] = firestore.DELETE_FIELD
            
            # ‡ß®. ‡¶°‡¶æ‡¶ü‡¶æ ‡¶á‡¶®‡¶ü‡ßá‡¶ó‡ßç‡¶∞‡¶ø‡¶ü‡¶ø ‡¶ö‡ßá‡¶ï
            data_updates = self.data_validator.validate_book(book_data)
            updates.update(data_updates)
            
            # ‡ß©. Last checked timestamp ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
            updates['last_checked'] = firestore.SERVER_TIMESTAMP
            updates['last_fixer_run'] = datetime.now().isoformat()
            
            # ‡ß™. Firestore ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ
            if updates:
                self.db.collection('books').document(doc_id).update(updates)
                self.stats.fixed_records += 1
                if self.config.verbose:
                    logger.info(f"üîß Updated {len(updates)} fields")
            else:
                self.stats.skipped_records += 1
            
            # Rate limiting (‡¶∂‡ßÅ‡¶ß‡ßÅ serial processing ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)
            if not self.config.enable_parallel:
                time.sleep(self.config.rate_limit_delay)
        
        except Exception as e:
            self.stats.errors += 1
            logger.error(f"‚ùå Error processing book: {e}")
    
    def _get_query(self) -> firestore.Query:
        """Smart filtering ‡¶∏‡¶π query ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ"""
        query = self.db.collection('books')
        
        # Smart filtering: ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶™‡ßÅ‡¶∞‡¶®‡ßã ‡¶∞‡ßá‡¶ï‡¶∞‡ßç‡¶° ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
        if self.config.enable_smart_filter:
            cutoff_date = datetime.now() - timedelta(days=self.config.check_interval_days)
            
            # ‡¶Ø‡ßá‡¶∏‡¶¨ ‡¶¨‡¶á ‡¶ó‡¶§ X ‡¶¶‡¶ø‡¶®‡ßá ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡¶®‡¶ø
            # Note: Firestore ‡¶è 'last_checked' field ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá‡¶ì query ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßá
            logger.info(f"üìÖ Filtering books not checked since {cutoff_date.strftime('%Y-%m-%d')}")
            query = query.where('last_checked', '<', cutoff_date)
        
        # Batch size limit
        if self.config.batch_size:
            query = query.limit(self.config.batch_size)
        
        return query
    
    def run_parallel(self, docs: List) -> None:
        """Parallel processing ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡¶á ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ"""
        logger.info(f"üöÄ Starting parallel processing with {self.config.max_workers} workers")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # ‡¶∏‡¶¨ docs ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø futures ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
            futures = []
            for doc in docs:
                if self._should_stop_execution():
                    break
                
                future = executor.submit(self.process_book, doc)
                futures.append(future)
                
                # Rate limiting: ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ submit ‡¶è‡¶∞ ‡¶™‡¶∞ ‡¶è‡¶ï‡¶ü‡ßÅ delay
                time.sleep(self.config.rate_limit_delay / self.config.max_workers)
            
            # ‡¶∏‡¶¨ futures complete ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶™‡ßá‡¶ï‡ßç‡¶∑‡¶æ
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"‚ùå Thread error: {e}")
                    self.stats.errors += 1
    
    def run_serial(self, docs: List) -> None:
        """Serial processing (‡¶è‡¶ï‡¶ü‡¶æ‡¶∞ ‡¶™‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ)"""
        logger.info(f"üîÑ Starting serial processing")
        
        for doc in docs:
            if self._should_stop_execution():
                break
            self.process_book(doc)
    
    def run(self) -> FixerStats:
        """‡¶Æ‡ßá‡¶á‡¶® execution function"""
        logger.info("üõ†Ô∏è  Fixer Bot started...")
        self.start_time = time.time()
        
        try:
            # Query ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
            query = self._get_query()
            
            # Documents fetch ‡¶ï‡¶∞‡¶æ
            docs = list(query.stream())
            logger.info(f"üìö Found {len(docs)} books to process")
            
            if len(docs) == 0:
                logger.info("‚ú® All books are up to date! No processing needed.")
                self.stats.execution_time = time.time() - self.start_time
                return self.stats
            
            # Parallel ‡¶¨‡¶æ Serial processing
            if self.config.enable_parallel and len(docs) > 5:
                self.run_parallel(docs)
            else:
                self.run_serial(docs)
            
            # Execution time calculate
            self.stats.execution_time = time.time() - self.start_time
            logger.info(f"‚è±Ô∏è  Execution completed in {self.stats.execution_time:.2f} seconds")
            
            # Final report
            self.stats.print_report()
            
            return self.stats
        
        except Exception as e:
            logger.error(f"‚ùå Critical error in Fixer Bot: {e}")
            raise
        
        finally:
            # Cleanup
            if hasattr(self, 'link_checker'):
                del self.link_checker

# ================================
# ‡¶Æ‡ßá‡¶á‡¶® ‡¶è‡¶ï‡ßç‡¶∏‡¶ø‡¶ï‡¶ø‡¶â‡¶∂‡¶®
# ================================

def main():
    """Main entry point"""
    global logger
    
    try:
        # Custom configuration
        config = FixerConfig(
            # Network settings
            request_timeout=15,
            retry_attempts=3,
            
            # Rate limiting
            rate_limit_delay=0.6,  # Archive.org ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
            
            # Parallel processing (GitHub Actions ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶™‡ßç‡¶ü‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡¶æ)
            max_workers=3,  # 3-4 ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶ö‡¶≤‡¶¨‡ßá
            enable_parallel=True,
            
            # Smart filtering (‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶™‡ßÅ‡¶∞‡¶®‡ßã ‡¶∞‡ßá‡¶ï‡¶∞‡ßç‡¶° ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ)
            check_interval_days=7,  # ‡¶ó‡¶§ ‡ß≠ ‡¶¶‡¶ø‡¶®‡ßá ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡¶®‡¶ø ‡¶è‡¶Æ‡¶® ‡¶¨‡¶á
            enable_smart_filter=True,
            
            # Batch processing
            batch_size=None,  # None = ‡¶∏‡¶¨ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶¨‡ßá
            max_execution_time=300,  # 5 minutes (GitHub Actions timeout ‡¶è‡¶∞ ‡¶Ü‡¶ó‡ßá‡¶á ‡¶•‡¶æ‡¶Æ‡¶¨‡ßá)
            
            # Data settings
            default_category="‡¶â‡¶™‡¶®‡ßç‡¶Ø‡¶æ‡¶∏",
            default_status=BookStatus.PUBLISHED.value,  # ‡¶´‡ßç‡¶∞‡¶®‡ßç‡¶ü‡¶è‡¶®‡ßç‡¶°‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
            
            # Logging
            verbose=True  # Detailed logs
        )
        
        # Logger initialize
        logger = setup_logging(config.verbose)
        
        # Fixer Bot ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã
        fixer = FixerBot(config)
        stats = fixer.run()
        
        # Exit code based on results
        if stats.errors > 0:
            logger.warning(f"‚ö†Ô∏è  Completed with {stats.errors} errors")
            exit(1)
        else:
            logger.info("‚úÖ Completed successfully")
            exit(0)
    
    except Exception as e:
        if logger:
            logger.error(f"‚ùå Fatal error: {e}")
        else:
            print(f"‚ùå Fatal error: {e}")
        exit(1)

if __name__ == "__main__":
    main()